{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grammar_Checker_Model_Usage.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOACIhUfJ6vm/aR7zYIbKDc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GDO-Galileo/do-voice-interaction/blob/error_correction/gdo_voicebot/grammar_correction_service/grammar_checker_model/Grammar_Checker_Model_Usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGktxftokMBz"
      },
      "source": [
        "# **Using the Trained Grammar Checker Model**\n",
        "\n",
        "This notebook descibes our method of using our custom Grammar Checker model (trained using our [Grammar_Checker_Model_Training](https://colab.research.google.com/drive/1_7RQQPkUHyF3ip5vCI0b2aOxSejOZcTv?usp=sharing) file).\n",
        "\n",
        "To convert predictions from out model into '1' (correct) and '0' (incorrect) acceptability labels, the outputted predictions must first go through a sigmoid function, then rounded to the nearest integer (which will either be '1' or '0'). Our model was trained with the `BCEWithLogitsLoss()` loss function, which adds a sigmoid activation function during training.\n",
        "\n",
        "Our model is specifically trained to predict for **lowercase** sentences **without punctuation except for apostropes** as is the output of the GDO's speech-to-text service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOU4AYWP4IqM"
      },
      "source": [
        "## **Preparation for Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xemvYjOEiXtZ"
      },
      "source": [
        "###################################\n",
        "#             Imports             #\n",
        "###################################\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pOugcuU2Yzq"
      },
      "source": [
        "###################################\n",
        "#      Upload Trained Model       #\n",
        "###################################\n",
        "\n",
        "MODEL_NAME = 'bert-base-uncased-GDO-trained.pth'\n",
        "\n",
        "# Upload:\n",
        "#   bert-base-uncased-GDO-trained.pth\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu6R7ZdGr1p3"
      },
      "source": [
        "### **The Model Class**\n",
        "\n",
        "This is a model class using the BERT base uncased model with an extra linear layer to give one output. For more information, see our [Grammar_Checker_Model_Training](https://colab.research.google.com/drive/1_7RQQPkUHyF3ip5vCI0b2aOxSejOZcTv#scrollTo=UGD2ncwqXuH4) file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPkTgKBYh6Pf"
      },
      "source": [
        "##################################\n",
        "#          Model Class           #\n",
        "##################################\n",
        "# with reference to https://stackoverflow.com/questions/64156202/add-dense-layer-on-top-of-huggingface-bert-model\n",
        "# and documentation at https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "\n",
        "class CustomBERTModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomBERTModel, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "    self.linear = torch.nn.Linear(768, 1)\n",
        "\n",
        "  # A forward pass through both the BERT model and linear layer\n",
        "  def forward(self, input_ids):\n",
        "    outputs = self.bert(input_ids, token_type_ids=None)\n",
        "\n",
        "    # Gets the ouput of the last hidden layer of the BERT model\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    linear_output = self.linear(last_hidden_states[:,0,:])\n",
        "\n",
        "    return linear_output\n",
        "\n",
        "  # Modified state_dict to only save linear layer weights and bias\n",
        "  def state_dict(self):\n",
        "    return self.linear.state_dict()\n",
        "\n",
        "  # Modified load_state_dict to only load linear layer weights and bias\n",
        "  def load_state_dict(self, state_dict):\n",
        "    self.linear.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLT1k6Jj2QbE"
      },
      "source": [
        "### **Loading the Model**\n",
        "\n",
        "The model's state dictionary is loaded from local storage and applied to our custom model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DioEuqOWikg7"
      },
      "source": [
        "def load_grammar_checker_model():\n",
        "  # Set device to 'CPU' since this is only for predictions\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "  # Load the uploaded model weights to the model class\n",
        "  grammar_checker = CustomBERTModel()\n",
        "  grammar_checker.load_state_dict(torch.load(MODEL_NAME, map_location=device))\n",
        "\n",
        "  # Set to evaluation mode to prepare for predictions\n",
        "  grammar_checker.eval()\n",
        "\n",
        "  return grammar_checker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LY6HnIp3xfJ"
      },
      "source": [
        "### **Data Tokenization**\n",
        "\n",
        "As in our [Grammar_Checker_Model_Training](https://colab.research.google.com/drive/1_7RQQPkUHyF3ip5vCI0b2aOxSejOZcTv#scrollTo=UGD2ncwqXuH4) file, tokenizing is done using the standard BERT base uncased tokenizer with the `do_lower_case` flag set to true (since this is the case for the GDO speech-to-text system)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzuFjSm8j8fM"
      },
      "source": [
        "##################################\n",
        "#           Tokenizer            #\n",
        "##################################\n",
        "\n",
        "def create_tokenizer():\n",
        "  return BertTokenizer.from_pretrained('bert-base-uncased', \n",
        "                                       do_lower_case = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JytYIh5U4OH3"
      },
      "source": [
        "## **Making Predictions**\n",
        "\n",
        "Making predictions is done similarly to testing. In this case, we use the `check_GE` function to go through a \"batch\" of sentences. Like previously, the sentences are tokenized, padded and converted to numerical tokens to be inputted. After this, a forward step is made and the predictions are converted to acceptability labels of '1' for a correct prediction and '0' for an incorrect prediction using sigmoid and rounding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lc_7mhgivc2"
      },
      "source": [
        "# Sigmoid function for use in converting our model's preditions\n",
        "#   to '0' and '1' tags\n",
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "\n",
        "# Check for a grammatical error using the grammar_checker model\n",
        "#   Takes in a list of sentences for which it will predict a\n",
        "#   '0' for an INCORRECT or '1' for a CORRECT sentence\n",
        "def check_GE(sents, checker_model, tokenizer):\n",
        "\n",
        "  ## Prepare Sentences for Input ##\n",
        "\n",
        "  # Tokenize each inputted sentence\n",
        "  tokenized_texts = [tokenizer.tokenize(str(sent)) for sent in sents]\n",
        "\n",
        "  # Padding sentences to the maximum length sentence\n",
        "  padded_sequence = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]\n",
        "  max_len = max([len(txt) for txt in padded_sequence])\n",
        "\n",
        "  # Pad the input tokens\n",
        "  input_ids = pad_sequences(padded_sequence, maxlen=max_len, dtype=\"long\",\n",
        "                            truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  prediction_inputs = torch.tensor(input_ids)\n",
        "\n",
        "\n",
        "  ## Make Predictions ##\n",
        "\n",
        "  # Don't compute or store gradients\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate predictions\n",
        "    logits = checker_model(prediction_inputs)\n",
        "\n",
        "  # Move predictions to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "\n",
        "  # To calculate the prediction, use sigmoid (since BCEWithLogitsLoss\n",
        "  #   was used for training) and then round to the nearest integer ('0' or '1')\n",
        "  predictions = np.rint(sigmoid(logits))\n",
        "\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cJ1xlQ-7iJn"
      },
      "source": [
        "## **Using `check_GE` to Predict Grammatical Errors**\n",
        "\n",
        "Below is an example of how `check_GE` may be used to make predictions for sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yJiuZFjiwZA"
      },
      "source": [
        "INPUT_SENTENCES = [\n",
        "    \"the grammar correction on\",\n",
        "    \"is the grammar correction on\",\n",
        "    \"is grammar correction on\",\n",
        "    \"turn on grammar correct\",\n",
        "    \"turn on grammar correction\",\n",
        "    \"the laboratory display are pretty good\",\n",
        "    \"the laboratory displays are pretty good\",\n",
        "    \"the laboratory displays are pretty match\",\n",
        "    \"displays are pretty good\"\n",
        "]\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"Loading tokeniser...\")\n",
        "tokenizer = create_tokenizer()\n",
        "\n",
        "# Load trained model\n",
        "print(\"Loading checker_model...\")\n",
        "checker_model = load_grammar_checker_model()\n",
        "\n",
        "# Check sentences with check_GE\n",
        "print(\"Checking sentences...\")\n",
        "predictions = check_GE(INPUT_SENTENCES, checker_model, tokenizer)\n",
        "\n",
        "# Print sentences next to predictions\n",
        "for i in range(len(INPUT_SENTENCES)):\n",
        "    print(\" \" + str(INPUT_SENTENCES[i]) + \"\\t\" + str(predictions[i][0]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}